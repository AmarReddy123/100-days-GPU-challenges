{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOIH2qZ1IWIA",
        "outputId": "ecd00895-3cd4-407b-bb4f-22132289192f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MatrixTranspose.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile MatrixTranspose.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "// Define the size of the matrix\n",
        "#define WIDTH 1024\n",
        "#define HEIGHT 1024\n",
        "\n",
        "// CUDA kernel for matrix transposition\n",
        "__global__ void transposeMatrix(const float* input, float* output, int width, int height) {\n",
        "    // Calculate the row and column index of the element\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    // Perform the transposition if within bounds\n",
        "    if (x < width && y < height) {\n",
        "        int inputIndex = y * width + x;\n",
        "        int outputIndex = x * height + y;\n",
        "        output[outputIndex] = input[inputIndex];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Host function to check for CUDA errors\n",
        "void checkCudaError(const char* message) {\n",
        "    cudaError_t error = cudaGetLastError();\n",
        "    if (error != cudaSuccess) {\n",
        "        std::cerr << message << \" - CUDA Error: \" << cudaGetErrorString(error) << std::endl;\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = WIDTH;\n",
        "    int height = HEIGHT;\n",
        "\n",
        "    // Allocate host memory\n",
        "    size_t size = width * height * sizeof(float);\n",
        "    float* h_input = (float*)malloc(size);\n",
        "    float* h_output = (float*)malloc(size);\n",
        "\n",
        "    // Initialize the input matrix with some values\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_input[i] = static_cast<float>(i);\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    float* d_input;\n",
        "    float* d_output;\n",
        "    cudaMalloc((void**)&d_input, size);\n",
        "    cudaMalloc((void**)&d_output, size);\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "    checkCudaError(\"Failed to copy input data to device\");\n",
        "\n",
        "    // Define block and grid sizes\n",
        "    dim3 blockSize(32, 32);\n",
        "    dim3 gridSize((width + blockSize.x - 1) / blockSize.x, (height + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "    // Launch the kernel\n",
        "    transposeMatrix<<<gridSize, blockSize>>>(d_input, d_output, width, height);\n",
        "    cudaDeviceSynchronize();\n",
        "    checkCudaError(\"Kernel execution failed\");\n",
        "\n",
        "    // Copy the result back to the host\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "    checkCudaError(\"Failed to copy output data to host\");\n",
        "\n",
        "    // Verify the result\n",
        "    bool success = true;\n",
        "    for (int i = 0; i < width; i++) {\n",
        "        for (int j = 0; j < height; j++) {\n",
        "            if (h_output[i * height + j] != h_input[j * width + i]) {\n",
        "                success = false;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    std::cout << (success ? \"Matrix transposition succeeded!\" : \"Matrix transposition failed!\") << std::endl;\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    }
  ]
}